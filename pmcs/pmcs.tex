\documentclass[twoside]{amsart}
\usepackage{fullpage}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\usepackage{lmodern}

% Set this to true for English, false for Slovene
\newif\ifenglish
\englishtrue  % Change to \englishtrue for English

\ifenglish
  % English language support
  \usepackage[english]{babel}
\else
  % Slovenian language support
  \usepackage[slovene]{babel}  % Commennt if Slovenian support is unavailable
\fi

\usepackage{hyperref}
\usepackage{amsmath,amssymb,amsfonts,mathtools}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{algorithm}
\usepackage{algpseudocode}

\linespread{1.2}

% Custom commands
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\providecommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\probP}{\mathrm{I\kern-0.15em P}}
\newcommand{\E}{\mathbb{E}}

% Custom commands for proof pages
\newcommand{\proofpage}{\clearpage\ifodd\value{page}\null\clearpage\fi}
\newcommand{\statementpage}{\clearpage\ifodd\value{page}\else\null\clearpage\fi}

% Theorem environments - upright text
\theoremstyle{definition}
\ifenglish
  \newtheorem{definition}{Definition}[section]
  \newtheorem{example}[definition]{Example}
  \newtheorem{remark}[definition]{Remark}
  \renewcommand\endexample{\hfill$\diamondsuit$}
\else
  \newtheorem{definicija}{Definicija}[section]
  \newtheorem{primer}[definicija]{Primer}
  \newtheorem{opomba}[definicija]{Opomba}
  \renewcommand\endprimer{\hfill$\diamondsuit$}
\fi

% Theorem environments - italic text
\theoremstyle{plain}
\ifenglish
  \newtheorem{lemma}[definition]{Lemma}
  \newtheorem{theorem}[definition]{Theorem}
  \newtheorem{proposition}[definition]{Proposition}
  \newtheorem{corollary}[definition]{Corollary}
\else
  \newtheorem{lema}[definicija]{Lema}
  \newtheorem{izrek}[definicija]{Izrek}
  \newtheorem{trditev}[definicija]{Trditev}
  \newtheorem{posledica}[definicija]{Posledica}
\fi

\newcommand\Vtextvisiblespace[1][.3em]{%
\mbox{\kern.06em\vrule height.3ex}%
\vbox{\hrule width#1}%
\hbox{\vrule height.3ex}}

\title{Probabilistic methods in computer science}
\author{Janez Podlogar}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\statementpage

\section{Discrete probability}

\subsection{Probability}

\begin{definition}
    A \emph{sample space} $\Omega$ is a set whose elements represent all possible outcomes
    of a random experiment.
\end{definition}

\begin{definition}
    Let $\Omega$ be a sample space. Subset $\mathcal{F} \subset \mathcal{P}(\Omega)$ is 
    a \emph{$\sigma$-algebra on $\Omega$} if
    \begin{itemize}
        \item $\Omega \in \mathcal{F}$,
        \item $A \in \mathcal{F} \implies A^\mathsf{c} \in \mathcal{F}$,
        \item $A_1, A_2, \ldots \in \mathcal{F} \implies \bigcup_{i=1}^\infty A_i \in \mathcal{F}$.
    \end{itemize}
    A $\sigma$-algebra represents events which events are ``observable'' or ``measurable'' in our 
    probability model.
\end{definition}

\begin{definition}
    A countably infinite sequence of events $\{A_i\}_{i=1}^{\infty}$ is \emph{pairwise disjoint} if
    \[
        \forall i \neq j \colon A_i \cap A_j = \emptyset.
    \]
\end{definition}

\begin{definition}
    Let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega$. A \emph{probability function} is any
    $\probP \colon \mathcal{F} \to [0,1]$ that satisfies
    \begin{itemize}
        \item $\forall A \in \mathcal{F} \colon \probP(A) \geq 0$,
        \item $\probP(\Omega) = 1$,
        \item for any countably infinite sequence of pairwise disjoint events
        $\{A_i\}_{i=1}^\infty$
        \[
            \probP\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^{\infty} \probP(A_i).
        \]
    \end{itemize}
\end{definition}

\begin{definition}
    The triple $(\Omega, \mathcal{F}, \probP)$ is called a probability space.
\end{definition}

We will use discrete probability spaces. In a discrete probability space, the sample space $\Omega$ 
is finite or countably infinite, and the $\sigma$-algebra $\mathcal{F}$ consists of all subsets
of $\Omega$. The probability function is uniquely determined by the probabilities of singleton
events, meaning that for any $A \subseteq \Omega$,
\[
    \mathbb{P}(A) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}).
\]

\begin{lemma}
    For any two events $A$ and $B$,
    \[
        \probP(A \cup B) = \probP(A) + \probP(B) - \probP(A \cap B).
    \]
\end{lemma}

\begin{lemma}[Union bound]
    For any countably infinite sequence of events $\{A_i\}_{i=1}^\infty$,
    \[
        \probP\left(\bigcup_{i=1}^\infty A_i\right) \leq \sum_{i=1}^{\infty} \probP(A_i).
    \]
\end{lemma}

\begin{definition}
    Events $\{A_i\}_{i=1}^\infty$ are \emph{mutually independent} if  for every finite subset 
    $\{i_1, i_2, \ldots, i_k\} \subseteq \N$
    \[
        \probP\left(\bigcap_{j=1}^k A_{i_j}\right) = \prod_{j=1}^k \probP(A_{i_j}).
    \]
\end{definition}

\proofpage

\begin{example}\label{ex:fair die}
    Consider a fair six-sided die throw where we can observe the exact outcome.
    We define the probability space $(\Omega, \mathcal{F}, \probP)$ where
    \begin{align*}
        &\Omega = \{1,2,3,4,5,6\}, \\
        &\mathcal{F} = \mathcal{P}, \\
        &\probP(A) = \frac{\abs{A}}{6}.
    \end{align*}
    For our $\sigma$-algebra we choose the full full power set, because have complete information 
    about the die outcome. Every possible subset of $\Omega$ represents a question we can 
    definitively answer after observing the roll. Observable events include:
    \begin{itemize}
        \item $\{5\}$; did we roll a five?
        \item $\{1,2,3,4\}$; did we roll less than five?
        \item $\{2,4,6\}$; did we roll an odd number?
    \end{itemize}
    All of these events are measurable and can be assigned probabilities.
\end{example}

\begin{example}
    Consider the same die throw, but now we can only observe whether the outcome is odd or even.
    Now the probability space $(\Omega, \mathcal{F}, \probP)$ is
    \begin{align*}
        &\Omega = \{1,2,3,4,5,6\}, \\
        &\mathcal{F} = \{\emptyset, \{1,3,5\}, \{2,4,6\}, \Omega\}, \\
        &\probP(A) = 
            \begin{cases}
                0 & \text{if } A = \emptyset, \\
                \frac{1}{2} & \text{if } A = \{1,3,5\}, \\
                \frac{1}{2} & \text{if } A = \{2,4,6\}, \\
                1 & \text{if } A = \Omega.
            \end{cases}
    \end{align*}
    We chose a coarse because our observational capability is limited to parity. This models 
    constraints on observation (limited measurement precision) or information available 
    at a given time (filtrations in stochastic processes).
\end{example}

\statementpage

\subsection{Discrete random variables}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \probP)$ be a probability space. 
    A \emph{discrete random variable} is a function $X \colon \Omega \to \mathbb{Z}$ 
    that is measurable with respect to the $\sigma$-algebra $\mathcal{F}$. 
    That is, for every value $c \in \mathbb{Z}$, the set 
    $\{\omega \in \Omega \mid X(\omega) = c\} \in \mathcal{F}$.
    We denote the probability of that eveny by
    \[
        \probP(X = c) = \sum_{s \in \Omega \colon X(s) = c} \probP(s).
    \]
\end{definition}

\begin{definition}
    Discrete random variables $\{X_i\}_{i=1}^\infty$ are \emph{mutually independent} if 
    for every finite subset $\{i_1, i_2, \ldots, i_k\} \subseteq \N$ and for all values 
    $c_1, c_2, \ldots, c_k \in \mathbb{Z}$,
    \[
        \probP(X_{i_1} = c_1, X_{i_2} = c_2, \ldots, X_{i_k} = c_k) 
        = \prod_{j=1}^k \probP(X_{i_j} = c_j).
    \]
\end{definition}

\begin{definition}
    The \emph{expectation of a discrete random variable $X$} is
    \[
        \E[X] = \sum_{x \in \mathbb{Z}} x \probP(X = x),
    \]
    The expectation is \emph{finite} if $\sum_{x \in \mathbb{Z}} \abs{x} \probP(X = x) $ 
    converges, otherwise it is \emph{unbound}.
\end{definition}

\begin{proposition}
    For any function $f \colon \mathbb{Z} \to \R$,
    \[
        \E[f(X)] = \E[X] = \sum_{x \in \mathbb{Z}} f(x) \probP(X = x).
    \]
\end{proposition}

\begin{lemma}
    For any constant $c$ and discrete random variable $X$
    \[
        \E[cX] = c \E[x].
    \]
\end{lemma}

\begin{theorem}[Linearity of expectation]
    For any finite collection of discrete random variables Discrete random variables
    $\{X_i\}_{i=1}^n$ with finite expectations
    \[
        \E \left[ \sum_{i=1}^n X_i \right] = \sum_{i=1}^n \E[X_i].
    \]
\end{theorem}

\begin{corollary}
    For any random variables $X, Y$ and constants $a,b \in \R$,
    \[
        \E[aX + bY] = a\E[X] + b\E[Y].
    \]
\end{corollary}

\begin{lemma}
    If random variables $X$ and $Y$ are independent, then
    \[
        \E[XY] = \E[X] \cdot \E[Y].
    \]
\end{lemma}

\begin{theorem}[Markov's inequality]
    For a non-negative random variable $X$ and $a \geq 0$,
    \[
        \probP(X \geq a) \leq \frac{\E[X]}{a}.
    \]
\end{theorem}

\proofpage

\begin{example}
    Consider the fair die from example~\ref{ex:fair die}. Define the random variable
    \[
        X(\omega) = \omega - 3
    \]
    which represents the deviation from the center value.
    Then $X$ maps $\Omega \to \{-2, -1, 0, 1, 2, 3\} \subset \mathbb{Z}$.
    
    For each value $c$, the preimage $\{\omega \in \Omega \mid X(\omega) = c\}$ is measurable
    \begin{align*}
        X^{-1}(-2) &= \{1\} \in \mathcal{F}, \quad \probP(X = -2) = \frac{1}{6}, \\
        X^{-1}(-1) &= \{2\} \in \mathcal{F}, \quad \probP(X = -1) = \frac{1}{6}, \\
        X^{-1}(0) &= \{3\} \in \mathcal{F}, \quad \probP(X = 0) = \frac{1}{6}.
    \end{align*}
    Similarly, $\probP(X = 1) = \probP(X = 2) = \probP(X = 3) = \frac{1}{6}$.
\end{example}

\begin{remark}
    For random variables, pairwise independence does not imply mutual independence. 
    Consider three random variables where $X_1$ and $X_2$ are independent fair coin flips and
    $X_3 = (X_1 + X_2) \bmod 2$. Therefore,
    \begin{align*}
        X_3 &= 0 \quad \text{if } X_1 = X_2, \\
        X_3 &= 1 \quad \text{if } X_1 \neq X_2
    \end{align*}
    
    Then $X_1, X_2, X_3$ are pairwise independent. As they satisfiy
    \[
        \forall i \neq j \colon \probP(X_i = a, X_j = b) = \probP(X_i = a) \cdot \probP(X_j = b).
    \]
    However, they are not mutually independent. Knowing any two variables 
    completely determines the third
    \[
        0 = \probP(X_1 = 0, X_2 = 0, X_3 = 1) \neq
        \probP(X_1 = 0) \cdot \probP(X_2 = 0) \cdot \probP(X_3 = 1) = \frac{1}{8}.
    \]
\end{remark}

\begin{remark}
    Linearity of expectation also holds for countabily infinte summations in certain cases.
    It can be shown that 
    \[
        \E \left[ \sum_{i=1}^\infty X_i \right] = \sum_{i=1}^\infty \E[X_i]
    \]
    whenever $\sum_{i=1}^\infty \E[\abs{X_i}]$ converges.
\end{remark}

\begin{example}
    The expected value of the sum of two fair dice is
    \[
        \E[X_1 + X_2] = \E[X_1] + \E[X_2] = 2\cdot \E[X_1] =
        2 \cdot \frac{1}{6}\sum_{i=1}^{6}i = 2 \cdot \frac{7}{2} = 7.
    \]
\end{example}

\statementpage

\section{QuickSort}

QuickSort is a simple sorting algorithm. The input are $n$ distinct numbers. A call to the funcion begins by 
choosing a \emph{pivot} element $p$. Then proceeds by comparing every other element to $a$,
dividing the elements into two sublists: those that are less and those that are greater than the $a$.
Quicksort then recurisvly sorts these sublists.

\begin{algorithm}
\caption{Quicksort}
    \begin{algorithmic}[1]
        \Require A set $S \subseteq \R$ with $\abs{S} = n$ distinct elements
        \Ensure A list containing the elements of $S$ in sorted order
        \If{$\abs{S} = 1$}
            \State \Return the single element of $S$
        \Else
            \State Choose $p \in S$ uniformly at random (the \emph{pivot})
            \State $S^- \gets \{a \in S \mid a < p\}$
            \State $S^+ \gets \{a \in S \mid a > p\}$
            \State \Return $\textsc{Quicksort}(S^-), p, \textsc{Quicksort}(S^+)$
        \EndIf
    \end{algorithmic}
\end{algorithm}

In the worst case, Quicksort runs in time $O(n^2)$, which happens when the pivot is always the smallest or largest element.
For example let's suppose or input has the form $x_1 = n, x_2 = n-1, \ldots, x_n = 1$ 
and we always choose the largest element as the pivot. The first pivot comparison requires $n-1$ comparisons. 
The divison has yielded one empty list (which requires no further work) and the sublist $x_2 = n-1, \ldots, x_n = 1$.
The next chosen pivot is $n-1$, which requires $n-2$ comparisons. Continuing in this fashion, QuickSort performs
\[
    (n-1) + (n-2) + \ldots + 1 = \frac{n(n-1)}{2}
\]
comparison.

We have clealy made a bad choice of pivots for the given input. A reasonable choice of pivots would require many fewer
comparisons. For example, if our pivot always divides the list into two nearly equal pieces, this means each recursive
call processes a list of half the size. Consequently, only $\log n$ nested calls can be made before reaching a list
of size $1$. In other words, the total number of comparisons $C(n)$ would obey the recurrence relation
\[
    C(n) \leq 2C\left(\frac{n}{2}\right) + O(n).
\]
The solution to this recurrence by the Master theorem is $C(n) = O(n \log n)$, which is the best possible result for
comparison-based sorting. 

\begin{theorem}\label{the:quicksort}
    Let $C(n)$ be the expected number of comparison made in QuickSort for $n$ elements. Then
    \[
        C(n) = O(n \log n).
    \]
\end{theorem}

\proofpage

\begin{proof}[First proof of Theorem~\ref{the:quicksort} by recursion]
    Clearly, $C(0) = C(1) = 0$. For $n > 1$, when we choose a pivot $a$, we make $n-1$ comparison. If the pivot is the $i$-th
    smallest element, then the two sublists are of size $\abs{S^-} = i-1$ and $\abs{S^+} = n-i$. Since each pivot has the same probability
    of being chosen, we have
    \[
        C(n) = \underbrace{(n-1)}_{\text{partition work}} + \underbrace{\frac{1}{n} \sum_{i=1}^n [C(i-1) + C(n-i)]}_{\text{recursive work}}
    \]
    We note that $C(s)$ appears twice in the summation for $s = 0, 1, \ldots, n-1$. For example when $n=6$, $C(2)$ appears 
    in the terms for $i=2$ and $i=4$. We can see this explicitly by changing the index of summation
    \begin{align*}
        C(n) &= (n-1) + \frac{1}{n} \left[ \sum_{i=1}^n C(i-1) + \sum_{i=1}^n C(n-i) \right] \\
        &= (n-1) + \frac{1}{n} \left[ \underbrace{\sum_{j=0}^{n-1} C(j)}_{\text{substitute } j=i-1} +
            \underbrace{\sum_{j=0}^{n-1} C(j)}_{\text{substitute } j=n-i}  \right] \\
        &= (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} C(j).
    \end{align*}
    We use induction to show that $C(n) \leq 5n \log n$.
    \begin{description}
        \item[Base case ($n=1$)] $C(1) = 0 \leq 5 \cdot 1 \cdot \log 1 = 0$.
        \item[Inductive step]
        \begin{align*}
            C(n) &= (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} C(j)
            \leq (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} 5j \log j \quad \text{(by inductive hypothesis)} \\
            &= (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j \log j +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log j \right) \\
            &\leq (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j \log \frac{n}{2} +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log n \right) \\
            &= (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j (\log n - 1) +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log n \right) \\
            &= (n-1) + \frac{10}{n} \left( - \sum_{j=1}^{\lfloor n/2 \rfloor} j 
            + \log n \sum_{j=1}^{n-1} j \right) \\
            &= (n-1) + \frac{10}{n} \left( - \frac{\frac{n}{2}(\frac{n}{2} + 1)}{2} + \frac{n(n-1)}{2} \log n \right) \\
            &= n - 1 - \frac{5n}{4} - \frac{5}{2} + 5n\log n - 5\log n \\
            &= 5n\log n - \frac{7}{2} - \frac{n}{4} - 5\log n \\
            &\leq 5n\log n \quad \text{(for } n \geq 2\text{)}
        \end{align*}
    \end{description}
\end{proof}

\proofpage

\begin{proof}[Second proof of Theorem~\ref{the:quicksort} by indicator variables for each comparison]
    Let $s_1, s_2, \ldots, s_n$ be the elements of $S$ in sorted order. For $i<j$, we define the indicator
    \[
        X_{ij} = 
        \begin{cases}
            1 & \text{if } s_i \text{ and } s_j \text{ are compared during QuickSort}, \\
            0 & \text{otherwise}.
        \end{cases}
    \]

    Two elements are compared if and only if one of them is chosen as a pivot when they're both
    still in the same subset during the recursion. Therefore, Quicksort compares each pair at
    most once. The total number of comparisons is
    \[
        X = \sum_{1 \leq i < j \leq n} X_{ij}.
    \]
    By linearity of expectation,
    \[
        \E[X] = \sum_{1 \leq i < j \leq n} \E{X_{ij}} = \sum_{1 \leq i < j \leq n} \probP(X_{ij} = 1).
    \]
    Let $S_{ij}$ be the last subset containing both $s_i$ and $s_j$ during the recursion.
    The elements $s_i$ and $s_j$ are compared if and only if the chosen pivot from $S_{ij}$
    is either $s_i$ or $s_j$. Since the pivot is chosen uniformly at random, this happens with probability
    \[
        \probP(X_{ij} = 1) = \frac{2}{\abs{S_{ij}}} \leq \frac{2}{j-i+1} 
    \]
    Because $S_{ij}$ is the last subset containing both $s_i$ and $s_j$, no element between them 
    could have been chosen as a pivot yet, otherwise $s_i$ and $s_j$ would have been separated,
    then $S_ij$ must contain at least all elements between them in sorted order. Therefore
    $\abs{S_{ij}} = j-i+1$. Thus,
    \begin{align*}
        \E[X] &\leq \sum_{1 \leq i < j \leq n} \frac{2}{j-i+1} \\
        &= 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{1}{j-i+1} \\
        &= 2 \underbrace{\sum_{i=1}^{n-1} \sum_{k=2}^{n-i+1} \frac{1}{k}}_{\text{substitute } k=j-i+1} \\
        &\leq 2 \sum_{i=1}^{n-1} \sum_{k=1}^{n} \frac{1}{k} \\
        &= 2 \sum_{i=1}^{n-1} H_n \\
        &\leq 2nH_n \\
        &\leq 2n(1 + \log n) = O(n \log n).
    \end{align*}
\end{proof}

\proofpage

\begin{proof}[Third proof of Theorem~\ref{the:quicksort} by number of subsets that contain a given element]
    For $s \in S$, let $X_s$ be the number of comparison involving $s$ when it is not a pivot.
    Then the total number of comparisons is
    \[
        X = \sum_{s \in S} X_s.
    \]
    For a fixed $s \in S$, let $S_1, S_2, \ldots, S_t$ be the sequence of subsets containing 
    $s$ during the recursion, where $S_1 = S$ and $S_t = \{s\}$ or $s$ is the pivot for $S_t$. 
    Therefore, $X_s = t-1$ because $s$ is compared once in each subset except the last one.

    Subset $S_i$ is of type $j$ if
    \[
        n \left( \frac{3}{4} \right)^{j+1} < \abs{S_i} \leq n \left( \frac{3}{4} \right)^j
    \]
    Type $j$ can only exist for $j = 0, 1, \ldots, \left\lfloor \log_{4/3}n \right\rfloor$,
    as the upper bound must be at least $n \left( \frac{3}{4} \right)^j \geq 1$. Since 
    $S_{i+1}$ is formed by choosing a pivot in $S_i$, $\abs{S_{i+1}} < \abs{S_i}$.
    
    What is the probability that $S_{i+1}$ is of type $j+1$ given that $S_i$ is of type $j$?
    Since the pivot is chosen uniformly at random, this happens if the pivot is not among the
    largest $\frac{1}{4}$ or smallest $\frac{1}{4}$ elements of $S_i$. Therefore,
    \[
        \probP(\text{$S_{i+1}$ is of type $(j+1)$} \mid \text{$S_i$ is of type $j$}) = \frac{1}{2}. 
    \]
    
    Let $t_j$ be the number of (consecutive) subsets of type $j$, then $t = \sum_j t_j$.
    The expected value of $t_j$ is the expected number of trials until the first success
    (moving to type $(j+1)$) in a sequence of Bernoulli trials with success probability $\frac{1}{2}$.
    Thus, $\E[t_j] = 2$ and 
    \[
    \E[t] = \sum_{j=0}^{\lfloor \log_{4/3} n \rfloor} \E[t_j] \leq 2 \log_{4/3} n + 2 = O(\log n).
    \]
    Finally,
    \[
        \E[X] = \sum_{s \in S} \E[X_s] = \sum_{s \in S} \left( \E[t] - 1 \right) =
        n (O(\log n) - 1) = O(n \log n).
    \]
\end{proof}

\proofpage

\begin{proof}[Fourth proof of Theorem~\ref{the:quicksort} by sets of different types according to the size]
    For $j \in \N \cup \{0\}$ define types of subsets as in the previous proof. 

    Fix type $j$. As Quicksort recursively processes subsets, it creates a tree structure 
    where each node represents a subset. Many nodes may be of type $j$, but we focus on 
    the \emph{first} sets of type $j$, the first time we encounter type $j$ in each branch.
    
    Formally, let $S_1, S_2, \ldots, S_u$ be all first sets of type $j$, where each $S_i$ is 
    characterized by:
    \begin{itemize}
        \item $S_i$ is of type $j$,
        \item No ancestor of $S_i$ in the recursion tree is of type $j$.
    \end{itemize}
    
    This means $S_i$ is the moment we enter type $j$ in its branch. If $S_i$ has descendants 
    that are also of type $j$, we do not count them as first sets since $S_i$ itself is already 
    their ancestor of type $j$.

    The sets $S_1, S_2, \ldots, S_u$ are pairwise disjoint. To see this, consider two 
    distinct first sets $S_i$ and $S_k$ of type $j$. Since both are first sets of type $j$, 
    neither is an ancestor of the other, so they lie in different branches of the recursion tree.
    At some point, a common ancestor was partitioned into disjoint subsets, with $S_i$ and $S_k$ 
    descending from different partitions. Since partitions remain disjoint throughout recursion, 
    $S_i \cap S_k = \emptyset$. Therefore,
    \[
        \sum_{i=1}^{u} \abs{S_i} \leq n.
    \]
    
    Let $X_j$ be the total number of comparisons in all first sets of type $j$ and their 
    descendants. For each first set $S_i$ of type $j$, when the pivot is chosen from the 
    middle half (probability $\frac{1}{2}$), the subset size decreases by at least a factor 
    of $\frac{3}{4}$, moving to type $j+1$ or beyond. By the same geometric distribution 
    argument as in the previous proof, the expected number of comparisons 
    for $S_i$ and all its descendants until leaving type $j$ is at most $2\abs{S_i}$.
    
    Therefore,
    \begin{align*}
        \E[X_j] &\leq \sum_{i=1}^{u} 2\abs{S_i} = 2\sum_{i=1}^{u} \abs{S_i} \leq 2n.
    \end{align*}
    
    Since there are $O(\log n)$ types,
    \[
        \E[\text{total comparisons}] = \sum_{j=0}^{\lfloor \log_{4/3} n \rfloor} \E[X_j] 
        \leq \sum_{j=0}^{\lfloor \log_{4/3} n \rfloor} 2n = O(n\log n).
    \]
\end{proof}

\proofpage

\begin{proof}[Fifth proof of Theorem~\ref{the:quicksort} by modifying Quicksort]
    We modify Quicksort to \emph{guarantee} good pivots by repeatedly sampling until we find one 
    where both partitions have size at most $\frac{3}{4}\abs{S}$.
    \begin{algorithm}
        \caption{Modified Quicksort}
        \begin{algorithmic}[1]
            \Require A set $S \subseteq \R$ with $\abs{S} = n$ distinct elements
            \Ensure A list containing the elements of $S$ in sorted order
            \If{$\abs{S} = 1$}
                \State \Return the single element of $S$
            \Else
                \Repeat
                    \State Choose $p \in S$ uniformly at random (the \emph{pivot})
                    \State $S^- \gets \{a \in S \mid a < p\}$
                    \State $S^+ \gets \{a \in S \mid a > p\}$
                \Until{$\abs{S^-}, \abs{S^+} \leq \frac{3n}{4}$}
                \State \Return $\textsc{Modified Quicksort}(S^-), p, \textsc{Modified Quicksort}(S^+)$
            \EndIf
        \end{algorithmic}
    \end{algorithm}
    The probability of choosing a good pivot is $\frac{1}{2}$, since half of the elements
    satisfy the condition. The expected number of iterations before we find a good pivot is $2$,
    because this is a sequence of Bernoulli trials with success probability $\frac{1}{2}$.
    Each iteration requires $n-1$ comparisons (comparing the chosen pivot to all other elements).

    Let $C(n)$ be the expected number of comparisons made by the Modified Quicksort. Then
    \begin{align*}
        C(n) &= \mathbb{E}[\text{\# iterations}] \cdot (n-1) + C(n_1) + C(n_2) \\
        &\leq 2n + C(n_1) + C(n_2),
    \end{align*}
    where $n_1 + n_2 = n-1$ and $n_1, n_2 \leq \frac{3n}{4}$ is guaranteed by a good pivot.
    Either by the Master theorem or by induction, we can show that $C(n) = O(n \log n)$. 
    Since this modified Quicksort makes more comparisons than the original Quicksort due to
    the repeated sampling, the original Quicksort also has expected $O(n \log n)$ comparisons.  
\end{proof}

\end{document}