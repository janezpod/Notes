\documentclass[twoside]{amsart}
\usepackage{fullpage}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\usepackage{lmodern}

% Set this to true for English, false for Slovene
\newif\ifenglish
\englishtrue  % Change to \englishtrue for English

\ifenglish
  % English language support
  \usepackage[english]{babel}
\else
  % Slovenian language support
  \usepackage[slovene]{babel}  % Commennt if Slovenian support is unavailable
\fi

\usepackage{hyperref}
\usepackage{amsmath,amssymb,amsfonts,mathtools}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{algorithm}
\usepackage{algpseudocode}

\linespread{1.2}

% Custom commands
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\providecommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\probP}{\mathrm{I\kern-0.15em P}}
\newcommand{\E}{\mathbb{E}}

% Custom commands for proof pages
\newcommand{\proofpage}{\clearpage\ifodd\value{page}\null\clearpage\fi}
\newcommand{\statementpage}{\clearpage\ifodd\value{page}\else\null\clearpage\fi}

% Theorem environments - upright text
\theoremstyle{definition}
\ifenglish
  \newtheorem{definition}{Definition}[section]
  \newtheorem{example}[definition]{Example}
  \newtheorem{remark}[definition]{Remark}
  \renewcommand\endexample{\hfill$\diamondsuit$}
\else
  \newtheorem{definicija}{Definicija}[section]
  \newtheorem{primer}[definicija]{Primer}
  \newtheorem{opomba}[definicija]{Opomba}
  \renewcommand\endprimer{\hfill$\diamondsuit$}
\fi

% Theorem environments - italic text
\theoremstyle{plain}
\ifenglish
  \newtheorem{lemma}[definition]{Lemma}
  \newtheorem{theorem}[definition]{Theorem}
  \newtheorem{proposition}[definition]{Proposition}
  \newtheorem{corollary}[definition]{Corollary}
\else
  \newtheorem{lema}[definicija]{Lema}
  \newtheorem{izrek}[definicija]{Izrek}
  \newtheorem{trditev}[definicija]{Trditev}
  \newtheorem{posledica}[definicija]{Posledica}
\fi

\newcommand\Vtextvisiblespace[1][.3em]{%
\mbox{\kern.06em\vrule height.3ex}%
\vbox{\hrule width#1}%
\hbox{\vrule height.3ex}}

\title{Probabilistic methods in computer science}
\author{Janez Podlogar}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\statementpage

\section{Discrete probability}

\subsection{Probability}

\begin{definition}
    A \emph{sample space} $\Omega$ is a set whose elements represent all possible outcomes
    of a random experiment.
\end{definition}

\begin{definition}
    Let $\Omega$ be a sample space. Subset $\mathcal{F} \subset \mathcal{P}(\Omega)$ is 
    a \emph{$\sigma$-algebra on $\Omega$} if
    \begin{itemize}
        \item $\Omega \in \mathcal{F}$,
        \item $A \in \mathcal{F} \implies A^\mathsf{c} \in \mathcal{F}$,
        \item $A_1, A_2, \ldots \in \mathcal{F} \implies \bigcup_{i=1}^\infty A_i \in \mathcal{F}$.
    \end{itemize}
    A $\sigma$-algebra represents events which events are ``observable'' or ``measurable'' in our 
    probability model.
\end{definition}

\begin{definition}
    A countably infinite sequence of events $\{A_i\}_{i=1}^{\infty}$ is \emph{pairwise disjoint} if
    \[
        \forall i \neq j \colon A_i \cap A_j = \emptyset.
    \]
\end{definition}

\begin{definition}
    Let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega$. A \emph{probability function} is any
    $\probP \colon \mathcal{F} \to [0,1]$ that satisfies
    \begin{itemize}
        \item $\forall A \in \mathcal{F} \colon \probP(A) \geq 0$,
        \item $\probP(\Omega) = 1$,
        \item for any countably infinite sequence of pairwise disjoint events
        $\{A_i\}_{i=1}^\infty$
        \[
            \probP\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^{\infty} \probP(A_i).
        \]
    \end{itemize}
\end{definition}

\begin{definition}
    The triple $(\Omega, \mathcal{F}, \probP)$ is called a probability space.
\end{definition}

We will use discrete probability spaces. In a discrete probability space, the sample space $\Omega$ 
is finite or countably infinite, and the $\sigma$-algebra $\mathcal{F}$ consists of all subsets
of $\Omega$. The probability function is uniquely determined by the probabilities of singleton
events, meaning that for any $A \subseteq \Omega$,
\[
    \mathbb{P}(A) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}).
\]

\begin{lemma}
    For any two events $A$ and $B$,
    \[
        \probP(A \cup B) = \probP(A) + \probP(B) - \probP(A \cap B).
    \]
\end{lemma}

\begin{lemma}[Union bound]
    For any countably infinite sequence of events $\{A_i\}_{i=1}^\infty$,
    \[
        \probP\left(\bigcup_{i=1}^\infty A_i\right) \leq \sum_{i=1}^{\infty} \probP(A_i).
    \]
\end{lemma}

\begin{definition}
    Events $\{A_i\}_{i=1}^\infty$ are \emph{mutually independent} if  for every finite subset 
    $\{i_1, i_2, \ldots, i_k\} \subseteq \N$
    \[
        \probP\left(\bigcap_{j=1}^k A_{i_j}\right) = \prod_{j=1}^k \probP(A_{i_j}).
    \]
\end{definition}

\proofpage

\begin{example}\label{ex:fair die}
    Consider a fair six-sided die throw where we can observe the exact outcome.
    We define the probability space $(\Omega, \mathcal{F}, \probP)$ where
    \begin{align*}
        &\Omega = \{1,2,3,4,5,6\}, \\
        &\mathcal{F} = \mathcal{P}, \\
        &\probP(A) = \frac{\abs{A}}{6}.
    \end{align*}
    For our $\sigma$-algebra we choose the full full power set, because have complete information 
    about the die outcome. Every possible subset of $\Omega$ represents a question we can 
    definitively answer after observing the roll. Observable events include:
    \begin{itemize}
        \item $\{5\}$; did we roll a five?
        \item $\{1,2,3,4\}$; did we roll less than five?
        \item $\{2,4,6\}$; did we roll an odd number?
    \end{itemize}
    All of these events are measurable and can be assigned probabilities.
\end{example}

\begin{example}
    Consider the same die throw, but now we can only observe whether the outcome is odd or even.
    Now the probability space $(\Omega, \mathcal{F}, \probP)$ is
    \begin{align*}
        &\Omega = \{1,2,3,4,5,6\}, \\
        &\mathcal{F} = \{\emptyset, \{1,3,5\}, \{2,4,6\}, \Omega\}, \\
        &\probP(A) = 
            \begin{cases}
                0 & \text{if } A = \emptyset, \\
                \frac{1}{2} & \text{if } A = \{1,3,5\}, \\
                \frac{1}{2} & \text{if } A = \{2,4,6\}, \\
                1 & \text{if } A = \Omega.
            \end{cases}
    \end{align*}
    We chose a coarse because our observational capability is limited to parity. This models 
    constraints on observation (limited measurement precision) or information available 
    at a given time (filtrations in stochastic processes).
\end{example}

\statementpage

\subsection{Discrete random variables}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \probP)$ be a probability space. 
    A \emph{discrete random variable} is a function $X \colon \Omega \to \mathbb{Z}$ 
    that is measurable with respect to the $\sigma$-algebra $\mathcal{F}$. 
    That is, for every value $c \in \mathbb{Z}$, the set 
    $\{\omega \in \Omega \mid X(\omega) = c\} \in \mathcal{F}$.
    We denote the probability of that eveny by
    \[
        \probP(X = c) = \sum_{s \in \Omega \colon X(s) = c} \probP(s).
    \]
\end{definition}

\begin{definition}
    Discrete random variables $\{X_i\}_{i=1}^\infty$ are \emph{mutually independent} if 
    for every finite subset $\{i_1, i_2, \ldots, i_k\} \subseteq \N$ and for all values 
    $c_1, c_2, \ldots, c_k \in \mathbb{Z}$,
    \[
        \probP(X_{i_1} = c_1, X_{i_2} = c_2, \ldots, X_{i_k} = c_k) 
        = \prod_{j=1}^k \probP(X_{i_j} = c_j).
    \]
\end{definition}

\begin{definition}
    The \emph{expectation of a discrete random variable $X$} is
    \[
        \E[X] = \sum_{x \in \mathbb{Z}} x \probP(X = x),
    \]
    The expectation is \emph{finite} if $\sum_{x \in \mathbb{Z}} \abs{x} \probP(X = x) $ 
    converges, otherwise it is \emph{unbound}.
\end{definition}

\begin{proposition}
    For any function $f \colon \mathbb{Z} \to \R$,
    \[
        \E[f(X)] = \E[X] = \sum_{x \in \mathbb{Z}} f(x) \probP(X = x).
    \]
\end{proposition}

\begin{lemma}
    For any constant $c$ and discrete random variable $X$
    \[
        \E[cX] = c \E[x].
    \]
\end{lemma}

\begin{theorem}[Linearity of expectation]
    For any finite collection of discrete random variables Discrete random variables
    $\{X_i\}_{i=1}^n$ with finite expectations
    \[
        \E \left[ \sum_{i=1}^n X_i \right] = \sum_{i=1}^n \E[X_i].
    \]
\end{theorem}

\begin{corollary}
    For any random variables $X, Y$ and constants $a,b \in \R$,
    \[
        \E[aX + bY] = a\E[X] + b\E[Y].
    \]
\end{corollary}

\begin{lemma}
    If random variables $X$ and $Y$ are independent, then
    \[
        \E[XY] = \E[X] \cdot \E[Y].
    \]
\end{lemma}

\begin{theorem}[Markov's inequality]
    For a non-negative random variable $X$ and $a \geq 0$,
    \[
        \probP(X \geq a) \leq \frac{\E[X]}{a}.
    \]
\end{theorem}

\proofpage

\begin{example}
    Consider the fair die from example~\ref{ex:fair die}. Define the random variable
    \[
        X(\omega) = \omega - 3
    \]
    which represents the deviation from the center value.
    Then $X$ maps $\Omega \to \{-2, -1, 0, 1, 2, 3\} \subset \mathbb{Z}$.
    
    For each value $c$, the preimage $\{\omega \in \Omega \mid X(\omega) = c\}$ is measurable
    \begin{align*}
        X^{-1}(-2) &= \{1\} \in \mathcal{F}, \quad \probP(X = -2) = \frac{1}{6}, \\
        X^{-1}(-1) &= \{2\} \in \mathcal{F}, \quad \probP(X = -1) = \frac{1}{6}, \\
        X^{-1}(0) &= \{3\} \in \mathcal{F}, \quad \probP(X = 0) = \frac{1}{6}.
    \end{align*}
    Similarly, $\probP(X = 1) = \probP(X = 2) = \probP(X = 3) = \frac{1}{6}$.
\end{example}

\begin{remark}
    For random variables, pairwise independence does not imply mutual independence. 
    Consider three random variables where $X_1$ and $X_2$ are independent fair coin flips and
    $X_3 = (X_1 + X_2) \bmod 2$. Therefore,
    \begin{align*}
        X_3 &= 0 \quad \text{if } X_1 = X_2, \\
        X_3 &= 1 \quad \text{if } X_1 \neq X_2
    \end{align*}
    
    Then $X_1, X_2, X_3$ are pairwise independent. As they satisfiy
    \[
        \forall i \neq j \colon \probP(X_i = a, X_j = b) = \probP(X_i = a) \cdot \probP(X_j = b).
    \]
    However, they are not mutually independent. Knowing any two variables 
    completely determines the third
    \[
        0 = \probP(X_1 = 0, X_2 = 0, X_3 = 1) \neq
        \probP(X_1 = 0) \cdot \probP(X_2 = 0) \cdot \probP(X_3 = 1) = \frac{1}{8}.
    \]
\end{remark}

\begin{remark}
    Linearity of expectation also holds for countabily infinte summations in certain cases.
    It can be shown that 
    \[
        \E \left[ \sum_{i=1}^\infty X_i \right] = \sum_{i=1}^\infty \E[X_i]
    \]
    whenever $\sum_{i=1}^\infty \E[\abs{X_i}]$ converges.
\end{remark}

\begin{example}
    The expected value of the sum of two fair dice is
    \[
        \E[X_1 + X_2] = \E[X_1] + \E[X_2] = 2\cdot \E[X_1] =
        2 \cdot \frac{1}{6}\sum_{i=1}^{6}i = 2 \cdot \frac{7}{2} = 7.
    \]
\end{example}

\statementpage

\section{QuickSort}

QuickSort is a simple sorting algorithm. The input are $n$ distinct numbers. A call to the funcion begins by 
choosing a \emph{pivot} element $p$. Then proceeds by comparing every other element to $a$,
dividing the elements into two sublists: those that are less and those that are greater than the $a$.
Quicksort then recurisvly sorts these sublists.

\begin{algorithm}
\caption{Quicksort}
    \begin{algorithmic}[1]
        \Require A set $S \subseteq \R$ with $\abs{S} = n$ distinct elements
        \Ensure A list containing the elements of $S$ in sorted order
        \If{$\abs{S} = 1$}
            \State \Return the single element of $S$
        \Else
            \State Choose $p \in S$ uniformly at random (the \emph{pivot})
            \State $S^- \gets \{b \in S \mid b < a\}$
            \State $S^+ \gets \{b \in S \mid b > a\}$
            \State \Return $\textsc{Quicksort}(S^-), a, \textsc{Quicksort}(S^+)$
        \EndIf
    \end{algorithmic}
\end{algorithm}

In the worst case, Quicksort runs in time $O(n^2)$, which happens when the pivot is always the smallest or largest element.
For example let's suppose or input has the form $x_1 = n, x_2 = n-1, \ldots, x_n = 1$ 
and we always choose the largest element as the pivot. The first pivot comparison requires $n-1$ comparisons. 
The divison has yielded one empty list (which requires no further work) and the sublist $x_2 = n-1, \ldots, x_n = 1$.
The next chosen pivot is $n-1$, which requires $n-2$ comparisons. Continuing in this fashion, QuickSort performs
\[
    (n-1) + (n-2) + \ldots + 1 = \frac{n(n-1)}{2}
\]
comparison.

We have clealy made a bad choice of pivots for the given input. A reasonable choice of pivots would require many fewer
comparisons. For example, if our pivot always divides the list into two nearly equal pieces, this means each recursive
call processes a list of half the size. Consequently, only $\log n$ nested calls can be made before reaching a list
of size $1$. In other words, the total number of comparisons $C(n)$ would obey the recurrence relation
\[
    C(n) \leq 2C\left(\frac{n}{2}\right) + O(n).
\]
The solution to this recurrence by the Master theorem is $C(n) = O(n \log n)$, which is the best possible result for
comparison-based sorting. 

\begin{theorem}\label{the:quicksort}
    Let $C(n)$ be the expected number of comparison made in QuickSort for $n$ elements. Then
    \[
        C(n) = O(n \log n).
    \]
\end{theorem}

\proofpage

\begin{proof}[First proof of Theorem~\ref{the:quicksort} by recursion]
    Clearly, $C(0) = C(1) = 0$. For $n > 1$, when we choose a pivot $a$, we make $n-1$ comparison. If the pivot is the $i$-th
    smallest element, then the two sublists are of size $\abs{S^-} = i-1$ and $\abs{S^+} = n-i$. Since each pivot has the same probability
    of being chosen, we have
    \[
        C(n) = \underbrace{(n-1)}_{\text{partition work}} + \underbrace{\frac{1}{n} \sum_{i=1}^n [C(i-1) + C(n-i)]}_{\text{recursive work}}
    \]
    We note that $C(s)$ appears twice in the summation for $s = 0, 1, \ldots, n-1$. For example when $n=6$, $C(2)$ appears 
    in the terms for $i=2$ and $i=4$. We can see this explicitly by changing the index of summation
    \begin{align*}
        C(n) &= (n-1) + \frac{1}{n} \left[ \sum_{i=1}^n C(i-1) + \sum_{i=1}^n C(n-i) \right] \\
        &= (n-1) + \frac{1}{n} \left[ \underbrace{\sum_{j=0}^{n-1} C(j)}_{\text{substitute } j=i-1} +
            \underbrace{\sum_{j=0}^{n-1} C(j)}_{\text{substitute } j=n-i}  \right] \\
        &= (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} C(j).
    \end{align*}
    We use induction to show that $C(n) \leq 5n \log n$.
    \begin{description}
        \item[Base case ($n=1$)] $C(1) = 0 \leq 5 \cdot 1 \cdot \log 1 = 0$.
        \item[Inductive step ($n \to n+1$)]
        \begin{align*}
            C(n) &= (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} C(j)
            \leq (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} 5j \log j \quad \text{(by inductive hypothesis)} \\
            &= (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j \log j +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log j \right) \\
            &\leq (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j \log \frac{n}{2} +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log n \right) \\
            &= (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j (\log n - 1) +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log n \right) \\
            &= (n-1) + \frac{10}{n} \left( - \sum_{j=1}^{\lfloor n/2 \rfloor} j 
            + \log n \sum_{j=1}^{n-1} j \right) \\
            &= (n-1) + \frac{10}{n} \left( - \frac{\frac{n}{2}(\frac{n}{2} + 1)}{2} + \frac{n(n-1)}{2} \log n \right) \\
            &= n - 1 - \frac{5n}{4} - \frac{5}{2} + 5n\log n - 5\log n \\
            &= 5n\log n - \frac{7}{2} - \frac{n}{4} - 5\log n \\
            &\leq 5n\log n \quad \text{(for } n \geq 2\text{)}
        \end{align*}
    \end{description}

\end{proof}

\proofpage

\begin{proof}[Second proof of Theorem~\ref{the:quicksort} by indicator variables for each comparison]
    Let $s_1, s_2, \ldots, s_n$ be the elements of $S$ in sorted order. For $i<j$, we define the indicator
    \[
        X_{ij} = 
        \begin{cases}
            1 & \text{if } s_i \text{ and } s_j \text{ are compared during QuickSort}, \\
            0 & \text{otherwise}.
        \end{cases}
    \]

    Two elements are compared if and only if one of them is chosen as a pivot when they're both
    still in the same subset during the recursion. Therefore,
    Quicksort compares each pair at most once and. The total number of comparisons is
    \[
        X = \sum_{1 \leq i < j \leq n} X_{ij}.
    \]
    By linearity of expectation,
    \[
        \E[X] = \sum_{1 \leq i < j \leq n} \E{X_{ij}} = \sum_{1 \leq i < j \leq n} \probP(X_{ij} = 1).
    \]
    Let $S_{ij}$ be the last subset containing both $s_i$ and $s_j$ during the recursion.
    The elements $s_i$ and $s_j$ are compared if and only if the chosen pivot from $S_{ij}$
    is either $s_i$ or $s_j$. Since the pivot is chosen uniformly at random, this happens with probability
    \[
        \probP(X_{ij} = 1) = \frac{2}{\abs{S_{ij}}} \leq \frac{2}{j-i+1} 
    \]
    Because $S_{ij}$ is the last subset containing both $s_i$ and $s_j$, no element between them 
    could have been chosen as a pivot yet, otherwise $s_i$ and $s_j$ would have been separated,
    then $S_ij$ must contain at least all elements between them in sorted order. Therefore
    $\abs{S_{ij}} = j-i+1$. Thus,
    \begin{align*}
        \E[X] &\leq \sum_{1 \leq i < j \leq n} \frac{2}{j-i+1} \\
        &= 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{1}{j-i+1} \\
        &= 2 \underbrace{\sum_{i=1}^{n-1} \sum_{k+2}^{n-i+1} \frac{1}{k}}_{\text{substitute } k=j-i+1} \\
        &\leq 2 \sum_{i=1}^{n-1} \sum_{k=1}^{n} \frac{1}{k} \\
        &= 2 \sum_{i=1}^{n-1} H_n \\
        &\leq 2nH_n \\
        &\leq 2n(1 + \log n) = O(n \log n).
    \end{align*}
\end{proof}

\end{document}