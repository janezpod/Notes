\documentclass[twoside]{amsart}
\usepackage{fullpage}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\usepackage{lmodern}

% Set this to true for English, false for Slovene
\newif\ifenglish
\englishtrue  % Change to \englishtrue for English

\ifenglish
  % English language support
  \usepackage[english]{babel}
\else
  % Slovenian language support
  \usepackage[slovene]{babel}  % Commennt if Slovenian support is unavailable
\fi

\usepackage{hyperref}
\usepackage{amsmath,amssymb,amsfonts,mathtools}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{fancyhdr}
% Define page style with numbers only on odd pages
\fancypagestyle{main}{%
    \fancyhf{}  % Clear all headers and footers
    \fancyfoot[CO]{\thepage}  % Page number in center of odd pages only
    \renewcommand{\headrulewidth}{0pt}  % No header rule
    \renewcommand{\footrulewidth}{0pt}  % No footer rule
}
% Redefine plain style (used by \maketitle) to have no page number
\fancypagestyle{plain}{%
  \fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{tikz}
\usetikzlibrary{positioning, shapes, arrows}

\linespread{1.2}

% Custom commands
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\providecommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\probP}{\mathrm{I\kern-0.15em P}}
\newcommand{\E}{\mathbb{E}}

% Custom commands for proof pages
\newcommand{\proofpage}{\clearpage\ifodd\value{page}\null\clearpage\fi}
\newcommand{\statementpage}{\clearpage\ifodd\value{page}\else\null\clearpage\fi}

% Theorem environments - upright text
\theoremstyle{definition}
\ifenglish
  \newtheorem{definition}{Definition}[section]
  \newtheorem{example}[definition]{Example}
  \newtheorem{remark}[definition]{Remark}
  \renewcommand\endexample{\hfill$\diamondsuit$}
\else
  \newtheorem{definicija}{Definicija}[section]
  \newtheorem{primer}[definicija]{Primer}
  \newtheorem{opomba}[definicija]{Opomba}
  \renewcommand\endprimer{\hfill$\diamondsuit$}
\fi

% Theorem environments - italic text
\theoremstyle{plain}
\ifenglish
  \newtheorem{lemma}[definition]{Lemma}
  \newtheorem{theorem}[definition]{Theorem}
  \newtheorem{proposition}[definition]{Proposition}
  \newtheorem{corollary}[definition]{Corollary}
\else
  \newtheorem{lema}[definicija]{Lema}
  \newtheorem{izrek}[definicija]{Izrek}
  \newtheorem{trditev}[definicija]{Trditev}
  \newtheorem{posledica}[definicija]{Posledica}
\fi

\newcommand\Vtextvisiblespace[1][.3em]{%
\mbox{\kern.06em\vrule height.3ex}%
\vbox{\hrule width#1}%
\hbox{\vrule height.3ex}}

\title{Probabilistic methods in computer science}
\author{Janez Podlogar}
\date{\today}

\begin{document}
\pagestyle{main}

\maketitle
\thispagestyle{empty}

\tableofcontents
\newpage

\statementpage

\section{Discrete probability}

\subsection{Probability}

\begin{definition}
    A \emph{sample space} $\Omega$ is a set whose elements represent all possible outcomes
    of a random experiment.
\end{definition}

\begin{definition}
    Let $\Omega$ be a sample space. Subset $\mathcal{F} \subset \mathcal{P}(\Omega)$ is 
    a \emph{$\sigma$-algebra on $\Omega$} if
    \begin{itemize}
        \item $\Omega \in \mathcal{F}$,
        \item $A \in \mathcal{F} \implies A^\mathsf{c} \in \mathcal{F}$,
        \item $A_1, A_2, \ldots \in \mathcal{F} \implies \bigcup_{i=1}^\infty A_i \in \mathcal{F}$.
    \end{itemize}
    A $\sigma$-algebra represents events which events are ``observable'' or ``measurable'' in our 
    probability model.
\end{definition}

\begin{definition}
    A countably infinite sequence of events $\{A_i\}_{i=1}^{\infty}$ is \emph{pairwise disjoint} if
    \[
        \forall i \neq j \colon A_i \cap A_j = \emptyset.
    \]
\end{definition}

\begin{definition}
    Let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega$. A \emph{probability function} is any
    $\probP \colon \mathcal{F} \to [0,1]$ that satisfies
    \begin{itemize}
        \item $\forall A \in \mathcal{F} \colon \probP(A) \geq 0$,
        \item $\probP(\Omega) = 1$,
        \item for any countably infinite sequence of pairwise disjoint events
        $\{A_i\}_{i=1}^\infty$
        \[
            \probP\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^{\infty} \probP(A_i).
        \]
    \end{itemize}
\end{definition}

\begin{definition}
    The triple $(\Omega, \mathcal{F}, \probP)$ is called a probability space.
\end{definition}

We will use discrete probability spaces. In a discrete probability space, the sample space $\Omega$ 
is finite or countably infinite, and the $\sigma$-algebra $\mathcal{F}$ consists of all subsets
of $\Omega$. The probability function is uniquely determined by the probabilities of singleton
events, meaning that for any $A \subseteq \Omega$,
\[
    \mathbb{P}(A) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}).
\]

\begin{lemma}
    For any two events $A$ and $B$,
    \[
        \probP(A \cup B) = \probP(A) + \probP(B) - \probP(A \cap B).
    \]
\end{lemma}

\begin{lemma}[Union bound]
    For any countably infinite sequence of events $\{A_i\}_{i=1}^\infty$,
    \[
        \probP\left(\bigcup_{i=1}^\infty A_i\right) \leq \sum_{i=1}^{\infty} \probP(A_i).
    \]
\end{lemma}

\begin{definition}
    Events $\{A_i\}_{i=1}^\infty$ are \emph{mutually independent} if  for every finite subset 
    $\{i_1, i_2, \ldots, i_k\} \subseteq \N$
    \[
        \probP\left(\bigcap_{j=1}^k A_{i_j}\right) = \prod_{j=1}^k \probP(A_{i_j}).
    \]
\end{definition}

\proofpage

\begin{example}\label{ex:fair die}
    Consider a fair six-sided die throw where we can observe the exact outcome.
    We define the probability space $(\Omega, \mathcal{F}, \probP)$ where
    \begin{align*}
        &\Omega = \{1,2,3,4,5,6\}, \\
        &\mathcal{F} = \mathcal{P}, \\
        &\probP(A) = \frac{\abs{A}}{6}.
    \end{align*}
    For our $\sigma$-algebra we choose the full full power set, because have complete information 
    about the die outcome. Every possible subset of $\Omega$ represents a question we can 
    definitively answer after observing the roll. Observable events include:
    \begin{itemize}
        \item $\{5\}$; did we roll a five?
        \item $\{1,2,3,4\}$; did we roll less than five?
        \item $\{2,4,6\}$; did we roll an odd number?
    \end{itemize}
    All of these events are measurable and can be assigned probabilities.
\end{example}

\begin{example}
    Consider the same die throw, but now we can only observe whether the outcome is odd or even.
    Now the probability space $(\Omega, \mathcal{F}, \probP)$ is
    \begin{align*}
        &\Omega = \{1,2,3,4,5,6\}, \\
        &\mathcal{F} = \{\emptyset, \{1,3,5\}, \{2,4,6\}, \Omega\}, \\
        &\probP(A) = 
            \begin{cases}
                0 & \text{if } A = \emptyset, \\
                \frac{1}{2} & \text{if } A = \{1,3,5\}, \\
                \frac{1}{2} & \text{if } A = \{2,4,6\}, \\
                1 & \text{if } A = \Omega.
            \end{cases}
    \end{align*}
    We chose a coarse because our observational capability is limited to parity. This models 
    constraints on observation (limited measurement precision) or information available 
    at a given time (filtrations in stochastic processes).
\end{example}

\statementpage

\subsection{Discrete random variables}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \probP)$ be a probability space. 
    A \emph{discrete random variable} is a function $X \colon \Omega \to \mathbb{Z}$ 
    that is measurable with respect to the $\sigma$-algebra $\mathcal{F}$. 
    That is, for every value $c \in \mathbb{Z}$, the set 
    $\{\omega \in \Omega \mid X(\omega) = c\} \in \mathcal{F}$.
    We denote the probability of that eveny by
    \[
        \probP(X = c) = \sum_{s \in \Omega \colon X(s) = c} \probP(s).
    \]
\end{definition}

\begin{definition}
    Discrete random variables $\{X_i\}_{i=1}^\infty$ are \emph{mutually independent} if 
    for every finite subset $\{i_1, i_2, \ldots, i_k\} \subseteq \N$ and for all values 
    $c_1, c_2, \ldots, c_k \in \mathbb{Z}$,
    \[
        \probP(X_{i_1} = c_1, X_{i_2} = c_2, \ldots, X_{i_k} = c_k) 
        = \prod_{j=1}^k \probP(X_{i_j} = c_j).
    \]
\end{definition}

\begin{definition}
    The \emph{expectation of a discrete random variable $X$} is
    \[
        \E[X] = \sum_{x \in \mathbb{Z}} x \probP(X = x),
    \]
    The expectation is \emph{finite} if $\sum_{x \in \mathbb{Z}} \abs{x} \probP(X = x) $ 
    converges, otherwise it is \emph{unbound}.
\end{definition}

\begin{proposition}
    For any function $f \colon \mathbb{Z} \to \R$,
    \[
        \E[f(X)] = \E[X] = \sum_{x \in \mathbb{Z}} f(x) \probP(X = x).
    \]
\end{proposition}

\begin{lemma}
    For any constant $c$ and discrete random variable $X$
    \[
        \E[cX] = c \E[x].
    \]
\end{lemma}

\begin{theorem}[Linearity of expectation]
    For any finite collection of discrete random variables Discrete random variables
    $\{X_i\}_{i=1}^n$ with finite expectations
    \[
        \E \left[ \sum_{i=1}^n X_i \right] = \sum_{i=1}^n \E[X_i].
    \]
\end{theorem}

\begin{corollary}
    For any random variables $X, Y$ and constants $a,b \in \R$,
    \[
        \E[aX + bY] = a\E[X] + b\E[Y].
    \]
\end{corollary}

\begin{lemma}
    If random variables $X$ and $Y$ are independent, then
    \[
        \E[XY] = \E[X] \cdot \E[Y].
    \]
\end{lemma}

\begin{theorem}[Markov's inequality]
    For a non-negative random variable $X$ and $a \geq 0$,
    \[
        \probP(X \geq a) \leq \frac{\E[X]}{a}.
    \]
\end{theorem}

\proofpage

\begin{example}
    Consider the fair die from example~\ref{ex:fair die}. Define the random variable
    \[
        X(\omega) = \omega - 3
    \]
    which represents the deviation from the center value.
    Then $X$ maps $\Omega \to \{-2, -1, 0, 1, 2, 3\} \subset \mathbb{Z}$.
    
    For each value $c$, the preimage $\{\omega \in \Omega \mid X(\omega) = c\}$ is measurable
    \begin{align*}
        X^{-1}(-2) &= \{1\} \in \mathcal{F}, \quad \probP(X = -2) = \frac{1}{6}, \\
        X^{-1}(-1) &= \{2\} \in \mathcal{F}, \quad \probP(X = -1) = \frac{1}{6}, \\
        X^{-1}(0) &= \{3\} \in \mathcal{F}, \quad \probP(X = 0) = \frac{1}{6}.
    \end{align*}
    Similarly, $\probP(X = 1) = \probP(X = 2) = \probP(X = 3) = \frac{1}{6}$.
\end{example}

\begin{remark}
    For random variables, pairwise independence does not imply mutual independence. 
    Consider three random variables where $X_1$ and $X_2$ are independent fair coin flips and
    $X_3 = (X_1 + X_2) \bmod 2$. Therefore,
    \begin{align*}
        X_3 &= 0 \quad \text{if } X_1 = X_2, \\
        X_3 &= 1 \quad \text{if } X_1 \neq X_2
    \end{align*}
    
    Then $X_1, X_2, X_3$ are pairwise independent. As they satisfiy
    \[
        \forall i \neq j \colon \probP(X_i = a, X_j = b) = \probP(X_i = a) \cdot \probP(X_j = b).
    \]
    However, they are not mutually independent. Knowing any two variables 
    completely determines the third
    \[
        0 = \probP(X_1 = 0, X_2 = 0, X_3 = 1) \neq
        \probP(X_1 = 0) \cdot \probP(X_2 = 0) \cdot \probP(X_3 = 1) = \frac{1}{8}.
    \]
\end{remark}

\begin{remark}
    Linearity of expectation also holds for countabily infinte summations in certain cases.
    It can be shown that 
    \[
        \E \left[ \sum_{i=1}^\infty X_i \right] = \sum_{i=1}^\infty \E[X_i]
    \]
    whenever $\sum_{i=1}^\infty \E[\abs{X_i}]$ converges.
\end{remark}

\begin{example}
    The expected value of the sum of two fair dice is
    \[
        \E[X_1 + X_2] = \E[X_1] + \E[X_2] = 2\cdot \E[X_1] =
        2 \cdot \frac{1}{6}\sum_{i=1}^{6}i = 2 \cdot \frac{7}{2} = 7.
    \]
\end{example}

\statementpage

\section{QuickSort}

QuickSort is a simple sorting algorithm. The input are $n$ distinct numbers. A call to the funcion begins by 
choosing a \emph{pivot} element $p$. Then proceeds by comparing every other element to $a$,
dividing the elements into two sublists: those that are less and those that are greater than the $a$.
Quicksort then recurisvly sorts these sublists.

\begin{algorithm}
\caption{Quicksort}
    \begin{algorithmic}[1]
        \Require A set $S \subseteq \R$ with $\abs{S} = n$ distinct elements
        \Ensure A list containing the elements of $S$ in sorted order
        \If{$\abs{S} = 1$}
            \State \Return the single element of $S$
        \Else
            \State Choose $p \in S$ uniformly at random (the \emph{pivot})
            \State $S^- \gets \{a \in S \mid a < p\}$
            \State $S^+ \gets \{a \in S \mid a > p\}$
            \State \Return $\textsc{Quicksort}(S^-), p, \textsc{Quicksort}(S^+)$
        \EndIf
    \end{algorithmic}
\end{algorithm}

In the worst case, Quicksort runs in time $O(n^2)$, which happens when the pivot is always the smallest or largest element.
For example let's suppose or input has the form $x_1 = n, x_2 = n-1, \ldots, x_n = 1$ 
and we always choose the largest element as the pivot. The first pivot comparison requires $n-1$ comparisons. 
The divison has yielded one empty list (which requires no further work) and the sublist $x_2 = n-1, \ldots, x_n = 1$.
The next chosen pivot is $n-1$, which requires $n-2$ comparisons. Continuing in this fashion, QuickSort performs
\[
    (n-1) + (n-2) + \ldots + 1 = \frac{n(n-1)}{2}
\]
comparison.

We have clealy made a bad choice of pivots for the given input. A reasonable choice of pivots would require many fewer
comparisons. For example, if our pivot always divides the list into two nearly equal pieces, this means each recursive
call processes a list of half the size. Consequently, only $\log n$ nested calls can be made before reaching a list
of size $1$. In other words, the total number of comparisons $C(n)$ would obey the recurrence relation
\[
    C(n) \leq 2C\left(\frac{n}{2}\right) + O(n).
\]
The solution to this recurrence by the Master theorem is $C(n) = O(n \log n)$, which is the best possible result for
comparison-based sorting. 

\begin{theorem}\label{the:quicksort}
    Let $C(n)$ be the expected number of comparison made in QuickSort for $n$ elements. Then
    \[
        C(n) = O(n \log n).
    \]
\end{theorem}

\proofpage

\begin{proof}[First proof of Theorem~\ref{the:quicksort} by recursion]
    Clearly, $C(0) = C(1) = 0$. For $n > 1$, when we choose a pivot $a$, we make $n-1$ comparison. If the pivot is the $i$-th
    smallest element, then the two sublists are of size $\abs{S^-} = i-1$ and $\abs{S^+} = n-i$. Since each pivot has the same probability
    of being chosen, we have
    \[
        C(n) = \underbrace{(n-1)}_{\text{partition work}} + \underbrace{\frac{1}{n} \sum_{i=1}^n [C(i-1) + C(n-i)]}_{\text{recursive work}}
    \]
    We note that $C(s)$ appears twice in the summation for $s = 0, 1, \ldots, n-1$. For example when $n=6$, $C(2)$ appears 
    in the terms for $i=2$ and $i=4$. We can see this explicitly by changing the index of summation
    \begin{align*}
        C(n) &= (n-1) + \frac{1}{n} \left[ \sum_{i=1}^n C(i-1) + \sum_{i=1}^n C(n-i) \right] \\
        &= (n-1) + \frac{1}{n} \left[ \underbrace{\sum_{j=0}^{n-1} C(j)}_{\text{substitute } j=i-1} +
            \underbrace{\sum_{j=0}^{n-1} C(j)}_{\text{substitute } j=n-i}  \right] \\
        &= (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} C(j).
    \end{align*}
    We use induction to show that $C(n) \leq 5n \log n$.
    \begin{description}
        \item[Base case ($n=1$)] $C(1) = 0 \leq 5 \cdot 1 \cdot \log 1 = 0$.
        \item[Inductive step]
        \begin{align*}
            C(n) &= (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} C(j)
            \leq (n-1) + \frac{2}{n} \sum_{j=0}^{n-1} 5j \log j \quad \text{(by inductive hypothesis)} \\
            &= (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j \log j +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log j \right) \\
            &\leq (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j \log \frac{n}{2} +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log n \right) \\
            &= (n-1) + \frac{10}{n} \left( \sum_{j = 1}^{\lfloor n/2 \rfloor} j (\log n - 1) +
            \sum_{j= \lfloor n/2 \rfloor + 1}^{n-1}  j \log n \right) \\
            &= (n-1) + \frac{10}{n} \left( - \sum_{j=1}^{\lfloor n/2 \rfloor} j 
            + \log n \sum_{j=1}^{n-1} j \right) \\
            &= (n-1) + \frac{10}{n} \left( - \frac{\frac{n}{2}(\frac{n}{2} + 1)}{2} + \frac{n(n-1)}{2} \log n \right) \\
            &= n - 1 - \frac{5n}{4} - \frac{5}{2} + 5n\log n - 5\log n \\
            &= 5n\log n - \frac{7}{2} - \frac{n}{4} - 5\log n \\
            &\leq 5n\log n \quad \text{(for } n \geq 2\text{)}
        \end{align*}
    \end{description}
\end{proof}

\proofpage

\begin{proof}[Second proof of Theorem~\ref{the:quicksort} by indicator variables for each comparison]
    Let $s_1, s_2, \ldots, s_n$ be the elements of $S$ in sorted order. For $i<j$, we define the indicator
    \[
        X_{ij} = 
        \begin{cases}
            1 & \text{if } s_i \text{ and } s_j \text{ are compared during QuickSort}, \\
            0 & \text{otherwise}.
        \end{cases}
    \]

    Two elements are compared if and only if one of them is chosen as a pivot when they're both
    still in the same subset during the recursion. Therefore, Quicksort compares each pair at
    most once. The total number of comparisons is
    \[
        X = \sum_{1 \leq i < j \leq n} X_{ij}.
    \]
    By linearity of expectation,
    \[
        \E[X] = \sum_{1 \leq i < j \leq n} \E{X_{ij}} = \sum_{1 \leq i < j \leq n} \probP(X_{ij} = 1).
    \]
    Let $S_{ij}$ be the last subset containing both $s_i$ and $s_j$ during the recursion.
    The elements $s_i$ and $s_j$ are compared if and only if the chosen pivot from $S_{ij}$
    is either $s_i$ or $s_j$. Since the pivot is chosen uniformly at random, this happens with probability
    \[
        \probP(X_{ij} = 1) = \frac{2}{\abs{S_{ij}}} \leq \frac{2}{j-i+1} 
    \]
    Because $S_{ij}$ is the last subset containing both $s_i$ and $s_j$, no element between them 
    could have been chosen as a pivot yet, otherwise $s_i$ and $s_j$ would have been separated,
    then $S_ij$ must contain at least all elements between them in sorted order. Therefore
    $\abs{S_{ij}} = j-i+1$. Thus,
    \begin{align*}
        \E[X] &\leq \sum_{1 \leq i < j \leq n} \frac{2}{j-i+1} \\
        &= 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n \frac{1}{j-i+1} \\
        &= 2 \underbrace{\sum_{i=1}^{n-1} \sum_{k=2}^{n-i+1} \frac{1}{k}}_{\text{substitute } k=j-i+1} \\
        &\leq 2 \sum_{i=1}^{n-1} \sum_{k=1}^{n} \frac{1}{k} \\
        &= 2 \sum_{i=1}^{n-1} H_n \\
        &\leq 2nH_n \\
        &\leq 2n(1 + \log n) = O(n \log n).
    \end{align*}
\end{proof}

\proofpage

\begin{proof}[Third proof of Theorem~\ref{the:quicksort} by number of subsets that contain a given element]
    For $s \in S$, let $X_s$ be the number of comparison involving $s$ when it is not a pivot.
    Then the total number of comparisons is
    \[
        X = \sum_{s \in S} X_s.
    \]
    For a fixed $s \in S$, let $S_1, S_2, \ldots, S_t$ be the sequence of subsets containing 
    $s$ during the recursion, where $S_1 = S$ and $S_t = \{s\}$ or $s$ is the pivot for $S_t$. 
    Therefore, $X_s = t-1$ because $s$ is compared once in each subset except the last one.

    Subset $S_i$ is of type $j$ if
    \[
        n \left( \frac{3}{4} \right)^{j+1} < \abs{S_i} \leq n \left( \frac{3}{4} \right)^j
    \]
    Type $j$ can only exist for $j = 0, 1, \ldots, \left\lfloor \log_{4/3}n \right\rfloor$,
    as the upper bound must be at least $n \left( \frac{3}{4} \right)^j \geq 1$. Since 
    $S_{i+1}$ is formed by choosing a pivot in $S_i$, $\abs{S_{i+1}} < \abs{S_i}$.
    
    What is the probability that $S_{i+1}$ is of type $j+1$ given that $S_i$ is of type $j$?
    Since the pivot is chosen uniformly at random, this happens if the pivot is not among the
    largest $\frac{1}{4}$ or smallest $\frac{1}{4}$ elements of $S_i$. Therefore,
    \[
        \probP(\text{$S_{i+1}$ is of type $(j+1)$} \mid \text{$S_i$ is of type $j$}) = \frac{1}{2}. 
    \]
    
    Let $t_j$ be the number of (consecutive) subsets of type $j$, then $t = \sum_j t_j$.
    The expected value of $t_j$ is the expected number of trials until the first success
    (moving to type $(j+1)$) in a sequence of Bernoulli trials with success probability $\frac{1}{2}$.
    Thus, $\E[t_j] = 2$ and 
    \[
    \E[t] = \sum_{j=0}^{\lfloor \log_{4/3} n \rfloor} \E[t_j] \leq 2 \log_{4/3} n + 2 = O(\log n).
    \]
    Finally,
    \[
        \E[X] = \sum_{s \in S} \E[X_s] = \sum_{s \in S} \left( \E[t] - 1 \right) =
        n (O(\log n) - 1) = O(n \log n).
    \]
\end{proof}

\proofpage

\begin{proof}[Fourth proof of Theorem~\ref{the:quicksort} by sets of different types according to the size]
    For $j \in \N \cup \{0\}$ define types of subsets as in the previous proof. 

    Fix type $j$. As Quicksort recursively processes subsets, it creates a tree structure 
    where each node represents a subset. Many nodes may be of type $j$, but we focus on 
    the \emph{first} sets of type $j$, the first time we encounter type $j$ in each branch.
    
    Formally, let $S_1, S_2, \ldots, S_u$ be all first sets of type $j$, where each $S_i$ is 
    characterized by:
    \begin{itemize}
        \item $S_i$ is of type $j$,
        \item No ancestor of $S_i$ in the recursion tree is of type $j$.
    \end{itemize}
    
    This means $S_i$ is the moment we enter type $j$ in its branch. If $S_i$ has descendants 
    that are also of type $j$, we do not count them as first sets since $S_i$ itself is already 
    their ancestor of type $j$.

    The sets $S_1, S_2, \ldots, S_u$ are pairwise disjoint. To see this, consider two 
    distinct first sets $S_i$ and $S_k$ of type $j$. Since both are first sets of type $j$, 
    neither is an ancestor of the other, so they lie in different branches of the recursion tree.
    At some point, a common ancestor was partitioned into disjoint subsets, with $S_i$ and $S_k$ 
    descending from different partitions. Since partitions remain disjoint throughout recursion, 
    $S_i \cap S_k = \emptyset$. Therefore,
    \[
        \sum_{i=1}^{u} \abs{S_i} \leq n.
    \]
    
    Let $X_j$ be the total number of comparisons in all first sets of type $j$ and their 
    descendants. For each first set $S_i$ of type $j$, when the pivot is chosen from the 
    middle half (probability $\frac{1}{2}$), the subset size decreases by at least a factor 
    of $\frac{3}{4}$, moving to type $j+1$ or beyond. By the same geometric distribution 
    argument as in the previous proof, the expected number of comparisons 
    for $S_i$ and all its descendants until leaving type $j$ is at most $2\abs{S_i}$.
    
    Therefore,
    \begin{align*}
        \E[X_j] &\leq \sum_{i=1}^{u} 2\abs{S_i} = 2\sum_{i=1}^{u} \abs{S_i} \leq 2n.
    \end{align*}
    
    Since there are $O(\log n)$ types,
    \[
        \E[\text{total comparisons}] = \sum_{j=0}^{\lfloor \log_{4/3} n \rfloor} \E[X_j] 
        \leq \sum_{j=0}^{\lfloor \log_{4/3} n \rfloor} 2n = O(n\log n).
    \]
\end{proof}

\proofpage

\begin{proof}[Fifth proof of Theorem~\ref{the:quicksort} by modifying Quicksort]
    We modify Quicksort to \emph{guarantee} good pivots by repeatedly sampling until we find one 
    where both partitions have size at most $\frac{3}{4}\abs{S}$.
    \begin{algorithm}
        \caption{Modified Quicksort}
        \begin{algorithmic}[1]
            \Require A set $S \subseteq \R$ with $\abs{S} = n$ distinct elements
            \Ensure A list containing the elements of $S$ in sorted order
            \If{$\abs{S} = 1$}
                \State \Return the single element of $S$
            \Else
                \Repeat
                    \State Choose $p \in S$ uniformly at random (the \emph{pivot})
                    \State $S^- \gets \{a \in S \mid a < p\}$
                    \State $S^+ \gets \{a \in S \mid a > p\}$
                \Until{$\abs{S^-}, \abs{S^+} \leq \frac{3n}{4}$}
                \State \Return $\textsc{Modified Quicksort}(S^-), p, \textsc{Modified Quicksort}(S^+)$
            \EndIf
        \end{algorithmic}
    \end{algorithm}
    The probability of choosing a good pivot is $\frac{1}{2}$, since half of the elements
    satisfy the condition. The expected number of iterations before we find a good pivot is $2$,
    because this is a sequence of Bernoulli trials with success probability $\frac{1}{2}$.
    Each iteration requires $n-1$ comparisons (comparing the chosen pivot to all other elements).

    Let $C(n)$ be the expected number of comparisons made by the Modified Quicksort. Then
    \begin{align*}
        C(n) &= \mathbb{E}[\text{\# iterations}] \cdot (n-1) + C(n_1) + C(n_2) \\
        &\leq 2n + C(n_1) + C(n_2),
    \end{align*}
    where $n_1 + n_2 = n-1$ and $n_1, n_2 \leq \frac{3n}{4}$ is guaranteed by a good pivot.
    Either by the Master theorem or by induction, we can show that $C(n) = O(n \log n)$. 
    
    Since this modified Quicksort makes more comparisons than the original Quicksort due to
    the repeated sampling, the original Quicksort also has expected $O(n \log n)$ comparisons.  
\end{proof}

\statementpage

\section{Minimum Cut}

\begin{definition}
    Let $G=(V,E)$ be a graph with $\abs{V} = n$ vertices. A \emph{cut-set} is a subset of edges
    $C \subseteq E$ such that removing the edges in $C$ disconnects $G$ into two or more components.
    A \emph{min-cut} is a cut-set of smallest possible size.
\end{definition}
We wish to find a min-cut in a given graph. One way to do this is by using the following randomized
algorithm. The main operation in the algorithm is \emph{edge contraction}.
\begin{definition}
    \emph{Edge contration} on an edge $e = (u,v) \in E$ merges the two vertices $u$ and $v$
    into a single vertex, eliminating all self loops and edges connecting $u$ and $v$, 
    and retaining all other edges.
\end{definition}

The algotihm consists of $n-2$ iterations. In each iteration, the algorith picks an edge
from the existing edges in the graph and contracts it. Each iteration reduces the number of vertices
by one. After $n-2$ iterations, only two vertices remain. The algorithm
returns the set of edges between these two vertices as the cut-set.

\begin{algorithm}
    \caption{RandMinCut}
    \begin{algorithmic}[1]
        \Require Multigraph $G=(V,E)$ with $\abs{V} = n \geq 2$ vertices
        \Ensure A cut-set $C \subseteq E$
        \State $G_0 \leftarrow G$
        \State $i \leftarrow 0$
        \While{$\abs{V(G_i)} > 2$}
            \State Choose an edge $e \in E(G_i)$ uniformly at random
            \State $G_{i+1} \leftarrow$ contract $e$ in $G_i$ 
            \State $i \leftarrow i + 1$
        \EndWhile
        \State \Return $E(G_i)$
    \end{algorithmic}
\end{algorithm}

\begin{theorem}\label{the:randmincut}
    Let $G=(V,E)$ be a multigraph with $n$ vertices.
    The probability that RandMinCut returns a min-cut is at least $\frac{2}{n(n-1)}$.
\end{theorem}

The probabilty that RandMinCut that chooses the wrong edge to contract,
increases in each iteration. This is because as the algorithm proceeds, the number of edges
that do not belong to the min-cut decreases, while the number of edges in the min-cut remains
the same.

\begin{corollary}\label{co:randmincut}
    If we run the RandMinCut algorithm $n(n-1)$ times independently, the probability
    that at least one of the runs returns a min-cut is at least $1 - e^{-2} \approx 0.86$.
\end{corollary}

\proofpage

\begin{proof}[Proof of Theorem~\ref{the:randmincut}]
    Let $k$ be the size of a min-cut in $G$. The graph may have of several cut-sets of minimum
    size. We compute the probability of finding a particular min-cut $C$.

    Since $C$ is a cut-set in the graph, removal of the set $C$ partiotions the set of vertices
    into two sets, $S$ and $V \setminus S$, such that no edges are connecting vertices
    in $S$ to vertices in $V \setminus S$.

    RandMinCut returns the cut-set $C$ if and only if no edge in $C$ is contracted during
    the $n-2$ iterations. Since if we contract any edge in $C$, we merge a vertex in $S$
    with a vertex in $V \setminus S$. If we never contract an edge in $C$, then after $n-2$ iterations,
    there will be exactly two vertices remaining, one representing all vertices in $S$
    and the other representing all vertices in $V \setminus S$. The only edges remaining
    between these two vertices are those in $C$.

    Let $E_i$ be the event tat the edge contracted in iteration $i$ is not in $C$ and let
    $F_i = \bigcap_{j=1}^i E_j$ be the event that no edge in $C$ is contracted in the first
    $i$ iterations. We wish to compute $\probP(F_{n-2})$.

    Since the min-cut is of size $k$, each vertex in $G$ has degree at least $k$. 
    If any vertex had degree less than $k$, then the set of edges connected to that
    vertex would form a min-cut. Since eache vertex has at least $k$ edges, the graph must
    have at least $\frac{nk}{2}$ edges.

    We start by computing $\probP(E_1) = \probP(F_1)$. The first contracted edge is chosen uniformly
    at random from the set of all edges. The probabilty that we don't choose an edge of $C$ in the
    first iteration is
    \[
        \probP(F_1) \geq 1 - \frac{k}{\frac{nk}{2}} = 1 - \frac{2}{n}.
    \]
    Let us suppose that the first contraction did not eliminate an edge of $C$. Therefore,
    \[
        \probP(E_2 \mid F_1) \geq 1 - \frac{k}{\frac{(n-1)k}{2}} = 1 - \frac{2}{n-1}.
    \]
    Similarly,
    \[
        \probP(E_i \mid F_{i-1}) \geq 1 - \frac{k}{\frac{(n-i+1)k}{2}} = 1 - \frac{2}{n-i+1}.
    \]
    To compute $\probP(F_{n-2})$, we use the chain rule of conditional probability
    \begin{align*}
        \probP(F_{n-2}) &= \probP(E_{n-2} \cap F_{n-3}) \\
        &= \probP(E_{n-2} \mid F_{n-3}) \cdot \probP(F_{n-3}) \\
        &= \probP(E_{n-2} \mid F_{n-3}) \cdot \probP(E_{n-3} \mid F_{n-4}) 
        \cdot \ldots \cdot  \probP(E_2 \mid F_1) \cdot \probP(F_1) \\
        &\geq \prod_{i=1}^{n-2} \left( 1 - \frac{2}{n-i+1} \right) \\
        &= \prod_{i=1}^{n-2} \frac{n-i-1}{n-i+1}
        = \frac{n-2}{n} \cdot \frac{n-3}{n-1} \cdot \frac{n-4}{n-2}
        \cdot \ldots \cdot \frac{4}{6} \cdot \frac{3}{5} \cdot \frac{2}{4} \cdot \frac{1}{3}
        = \frac{2}{n(n-1)}.
    \end{align*}
\end{proof}

\proofpage

\begin{proof}[Proof of Corollary~\ref{co:randmincut}]
    Let $A_i$ be the event that the $i$-th run returns a min-cut.
    Since we run the algorithm independently and $\probP(A_i) \geq \frac{2}{n(n-1)}$, 
    \begin{align*}
        \probP(\text{no run returns a min-cut}) 
        &= \probP\left( \bigcap_i A_i^\mathsf{c} \right) \\
        &\leq \prod_i \probP(A_i^\mathsf{c}) \\
        &\leq \left( 1 - \frac{2}{n(n-1)} \right)^{n(n-1)} \\
        &\leq \left( e^{-\frac{2}{n(n-1)}} \right)^{n(n-1)} \quad \text{using } 1+x \leq e^x \text{ for all } x \in \R \\
        &= e^{-2} \approx 0.14.
    \end{align*}
\end{proof}

\begin{example}\label{ex:mincut}
    Consider the graph $G$ with vertices $\{1,2,3,4,5\}$ shown below. The minimum cut-set
    has size $k=2$, consisting of the edges $(3,5)$ and $(4,5)$ that separate vertex $5$ 
    from the rest of the graph.

    \begin{center}
    \begin{tikzpicture}[
        vertex/.style={circle, fill=black, inner sep=2pt},
        every node/.style={font=\small},
        scale=0.8
    ]

    % (a) A successful run of min-cut
    % Stage 1 - Original graph
    \begin{scope}[xshift=0cm]
        \node[vertex, label=above:1] (v1) at (0,1) {};
        \node[vertex, label=below:2] (v2) at (0,0) {};
        \node[vertex, label=above:3] (v3) at (1,1) {};
        \node[vertex, label=below:4] (v4) at (1,0) {};
        \node[vertex, label=right:5] (v5) at (2.5,0.5) {};
        
        \draw (v1) -- (v2);
        \draw (v1) -- (v3);
        \draw (v1) -- (v4);
        \draw (v2) -- (v3);
        \draw (v2) -- (v4);
        \draw (v3) -- (v4);
        \draw (v3) -- (v5);
        \draw (v4) -- (v5);
    \end{scope}

    % Stage 2
    \begin{scope}[xshift=4cm]
        \node[vertex, label=above:1] (v1) at (0,1) {};
        \node[vertex, label=below:2] (v2) at (0,0) {};
        \node[vertex, label=right:5] (v5) at (2.2,0.5) {};
        \node[vertex] (v34) at (1.1,0.5) {};
        \node at (1.1,0) {\tiny 3,4};
        
        \draw (v1) -- (v2);
        \draw (v1) to[bend left=20] (v34);
        \draw (v1) to[bend right=20] (v34);
        \draw (v2) to[bend left=20] (v34);
        \draw (v2) to[bend right=20] (v34);
        \draw (v34) to[bend left=20] (v5);
        \draw (v34) to[bend right=20] (v5);
    \end{scope}

    % Stage 3
    \begin{scope}[xshift=7.5cm]
        \node[vertex, label=below:2] (v2) at (0,0.5) {};
        \node[vertex, label=right:5] (v5) at (2.2,0.5) {};
        \node[vertex] (v134) at (1.1,0.5) {};
        \node at (1.1,0) {\tiny 1,3,4};
        
        \draw (v2) to[bend left=30] (v134);
        \draw (v2) to[bend right=30] (v134);
        \draw (v134) to[bend left=20] (v5);
        \draw (v134) to[bend right=20] (v5);
    \end{scope}

    % Stage 4
    \begin{scope}[xshift=11cm]
        \node[vertex, label=right:5] (v5) at (1.5,0.5) {};
        \node[vertex] (v1234) at (0,0.5) {};
        \node at (0,0) {\tiny 1,2,3,4};
        
        \draw (v1234) to[bend left=30] (v5);
        \draw (v1234) to[bend right=30] (v5);
    \end{scope}

    \node at (5.5, -1) {(a) A successful run of RandMinCut.};

    \end{tikzpicture}
    \end{center}

    \vspace{0.5cm}

    \begin{center}
    \begin{tikzpicture}[
        vertex/.style={circle, fill=black, inner sep=2pt},
        every node/.style={font=\small},
        scale=0.8
    ]

    % (b) An unsuccessful run of min-cut
    % Stage 1 - Original graph
    \begin{scope}[xshift=0cm]
        \node[vertex, label=above:1] (v1) at (0,1) {};
        \node[vertex, label=below:2] (v2) at (0,0) {};
        \node[vertex, label=above:3] (v3) at (1,1) {};
        \node[vertex, label=below:4] (v4) at (1,0) {};
        \node[vertex, label=right:5] (v5) at (2.5,0.5) {};
        
        \draw (v1) -- (v2);
        \draw (v1) -- (v3);
        \draw (v1) -- (v4);
        \draw (v2) -- (v3);
        \draw (v2) -- (v4);
        \draw (v3) -- (v4);
        \draw (v3) -- (v5);
        \draw (v4) -- (v5);
    \end{scope}

    % Stage 2
    \begin{scope}[xshift=4cm]
        \node[vertex, label=above:1] (v1) at (0,0.7) {};
        \node[vertex, label=below:2] (v2) at (0,0) {};
        \node[vertex, label=right:5] (v5) at (2.2,0.35) {};
        \node[vertex] (v34) at (1.1,0.35) {};
        \node at (1.1,-0.15) {\tiny 3,4};
        
        \draw (v1) -- (v2);
        \draw[line width=1.5pt] (v1) to[bend left=15] (v34);
        \draw (v1) to[bend right=15] (v34);
        \draw (v2) to[bend left=15] (v34);
        \draw (v2) to[bend right=15] (v34);
        \draw (v34) to[bend left=20] (v5);
        \draw (v34) to[bend right=20] (v5);
    \end{scope}

    % Stage 3
    \begin{scope}[xshift=7.5cm]
        \node[vertex, label=above:1] (v1) at (0,0.5) {};
        \node[vertex, label=below:2] (v2) at (0,0) {};
        \node[vertex] (v345) at (1.3,0.25) {};
        \node at (1.5,-0.2) {\tiny 3,4,5};
        
        \draw (v1) -- (v2);
        \draw (v1) to[bend left=20] (v345);
        \draw (v1) to[bend right=20] (v345);
        \draw (v2) to[bend left=20] (v345);
        \draw (v2) to[bend right=20] (v345);
    \end{scope}

    % Stage 4
    \begin{scope}[xshift=11cm]
        \node[vertex, label=above:1] (v1) at (0,0.3) {};
        \node[vertex] (v2345) at (1.3,0.3) {};
        \node at (1.5,-0.2) {\tiny 2,3,4,5};
        
        \draw (v1) to[bend left=40] (v2345);
        \draw (v1) to[bend right=40] (v2345);
    \end{scope}

    \node at (5.5, -1) {(b) An unsuccessful run of RandMinCut.};

    \end{tikzpicture}
    \end{center}
\end{example}

\end{document}